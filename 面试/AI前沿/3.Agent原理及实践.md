推荐阅读入门电子书籍：[HelloAgent](https://datawhalechina.github.io/hello-agents/)

---

# 原理篇

Agent基于LLM，此项技术可以让LLM具有与实际世界交互的功能！

Agent和人类的思维方式是类似的，从*感知*到*决策*再到*行动*，形成逻辑闭环。

按照**感知、规划、记忆、行动、反思**逻辑链进行内容处理及反应。

Agent会在收到用户的指令后，对用户的命令进行分析，分步骤，然后分步执行，这样可以使得用户的指令得到充分地理解，从而提高用户的满意程度。怎样对用户的指令进行分步骤，这采用的方法就是ReAct规范。

Agent可以分为两大类：

1. 反应型：快速、局部、简单
2. 规划型：动态、全局、复杂

这两类Agent没有谁更好，只有更适合什么场景！前者更适合只需要快速回复的场景（ai客服等），后者更适合需要一定复杂性的场景（旅游规划等）。

**ReAct**是一种结合推理和行动的智能体设计范式。

> 如果说LLM是一位**“只会背书的学者”**，那么ReAct模式就是一位**“带着笔记解题的研究员“**——它边思考边行动，通过**语言推理+工具调用**的循环逼近目标！

在不断地逻辑推理过程中，可以将LLM的思考过程输出，不仅可以减少幻觉的问题，还可以更好得处理结果错误的原因。

**Plan-and-Execute**是一种面向**复杂、多步骤、强依赖任务**的智能体架构范式。其核心思想是将决策过程明确划分为两个阶段。

1. Planner：负责从用户目标出发，生成一个结构化的、带依赖关系的高层任务计划
2. Executor：按计划逐步调用工具执行原子操作，并将执行结果实时反馈给Planner

通过**先全局规划→再分布执行→遇错动态调整**的闭环机制，Plan-and-Execute在保证任务逻辑严谨性的同时，具备一定的容错与自适应能力。

> 如果说ReAct是一位**“边走边想的探险家”**，那么Plan-and-Execute就是一位**“先画地图再行军的指挥家”**——它强调**事前规划**与**执行解耦**，适合流程明确但容错要求高的场景。

ReAct模式和Plan-and-Execute

|   维度   |                  ReAct模式                   |             Plan-and-Execute模式             |
| :------: | :------------------------------------------: | :------------------------------------------: |
| 决策节奏 |              逐步推理+及时行动               |            先全局规划→再批量执行             |
| 适用场景 |      信息不全、路径不确定、需要频繁探索      |       流程固定、依赖明确、需要事务保障       |
| 可解释性 | 展示每一步的思考过程，适合调试“为什么这么做” |    展示整体任务蓝图，适合理解“整体怎么做”    |
| 执行效率 |         较低（每一步都需要LLM推理）          | 较高（计划一次性生成，执行阶段无需反复推理） |
| 容错机制 |     依赖后续观察修正前序错误（隐式纠错）     |    显式建模失败路径与回滚策略（主动容错）    |
| 结构层级 |      算法层策略（LLM内部推理-行动循环）      |           系统层架构（组件化分工）           |

两种策略并非互斥，而是互补！系统可以采用**Plan-and-Execute**为主干，**ReAct**为叶子节点的混合架构。其中，Planner用结构化逻辑生成主流程，Executor在执行某个原子任务时，内部使用ReAct策略动态调用工具。

**Agent的演进之路！**

1. 传统AI时代
2. LLM时代
3. 底层支撑

阶段一：传统AI时代

1. 反应型

  - 核心原理：感知→触发

  - 原理：无记忆、无规划。输入直接触发预设行动（如机器人避障）
  - 痛点：无法处理长期目标或复杂任务

2. 规划型
  - 核心逻辑：目标→拆解→执行
  - 原理：先制定完整计划再执行
  - 痛点：计算开销大，环境变化会导致计划失效
3. 混合型
	- 核心逻辑：底层反应+上层规划+云端协作
	- 意义：LLM时代的底层逻辑。保留快速响应，叠加目标拆解能力
	- 应用：三层架构（反应层/规划层/协作层）

阶段二：LLM驱动的实现策略（大时代模型）

随着大模型的出现：**推理能力+工具使用能力**成为核心突破点。这一阶段的策略都是对**混合型范式**的具体落地。

1. Chain-of-Thought+Tool Use（基础起点）
2. Plan-and-Execute（规划优先）
3. ReAct（思考-行动循环）
4. Multi-Agent协作（规模扩展）

阶段三：底层技术组件（能力支撑）

这一阶段关注的是如何让 Agent 具备**记忆、知识库、外部交互通道**以及**安全边界**，是 Agent 从“一段代码”变成“一个系统”的关键。

1. 记忆管理 (Memory Management)
	- 短期记忆：基于 **Context Window**（上下文窗口）。利用大模型的缓存机制，存储当前的对话流和中间推理状态。
	
	- 长期记忆：基于 **向量数据库 (Vector DB)**。通过将历史经验或海量数据 Embedding（向量化）存储，让 Agent 在需要时能检索到很久以前的信息。

	- 意义：解决了大模型“转头就忘”的问题，实现了个性化和持续进化。
	
2. 知识检索增强 (RAG, Retrieval Augmented Generation)
	- 核心逻辑：检索 → 增强 → 生成。
	- 原理：Agent 不再只靠脑子里的训练数据，而是学会了“查字典”。在回答前先去私有文档或实时数据库中搜索最新信息。
	- 工具：LlamaIndex, LangChain 等框架提供的索引能力。

3. 统一交互协议与连接器 (Connectivity & MCP)
	- 核心组件：**MCP (Model Context Protocol)**、API 调用协议。
	- 原理：为 Agent 提供标准化的“手指”。无论底层的工具是 Google Drive、本地磁盘还是数据库，Agent 只要遵循协议就能无缝调用。
	- 意义：解决了 Agent 与第三方软件（SaaS/本地环境）的兼容性问题。

4. 环境与运行沙箱 (Environment & Sandboxing)
	- 核心逻辑：安全执行 + 物理触达。
	- 原理：为 Agent 提供一个安全的运行环境（如 Docker 容器、E2B 沙箱），让它在里面运行 Python 代码、抓取网页或处理文件，而不至于搞坏用户的真实系统。

5. 评估与观测 (AgentOps / Evaluation)
	- 核心逻辑：监控 → 评分 → 优化。
	- 原理：记录 Agent 的每一个步骤（Trace），分析哪一步推理出错了（比如逻辑幻觉），或者哪一步调用工具失败了。
	- 工具：LangSmith, Weights & Biases 等。

# 实践篇

